{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hack [nairaland.com](https://www.nairaland.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import codecs\n",
    "import logging\n",
    "\n",
    "from operator import itemgetter\n",
    "from itertools import filterfalse\n",
    "from collections import OrderedDict, namedtuple, Counter\n",
    "\n",
    "import bs4\n",
    "import docx\n",
    "import requests\n",
    "import requests.exceptions as rqe\n",
    "import openpyxl as OP\n",
    "\n",
    "from pywebber import Ripper\n",
    "\n",
    "USERHOME = os.path.abspath(os.path.expanduser('~'))\n",
    "DESKTOP = os.path.abspath(USERHOME + '/Desktop/')\n",
    "BASE_DIR = STATUS_DIR = os.path.join(DESKTOP ,\"hack-nairaland\")\n",
    "\n",
    "if not os.path.exists(BASE_DIR):\n",
    "    os.mkdir(BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Error(Exception):\n",
    "    pass\n",
    "\n",
    "class NonExistentNairalandUser(Error):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_logger(log_file_name):\n",
    "    FORMATTER = logging.Formatter(\"%(asctime)s:%(funcName)s:%(levelname)s\\n%(message)s\")\n",
    "    # console_logger = logging.StreamHandler(sys.stdout)\n",
    "    file_logger = logging.FileHandler(log_file_name)\n",
    "    file_logger.setFormatter(FORMATTER)\n",
    "\n",
    "    logger = logging.getLogger(log_file_name)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    logger.addHandler(file_logger)\n",
    "    logger.propagate = False\n",
    "    return logger\n",
    "\n",
    "PARSE_BR_TAG_LOGGER = new_logger('log_html_br_tag.log')\n",
    "PARSE_COMMENT_BLOCK_LOGGER = new_logger('log_parse_comment_block.log')\n",
    "FORMAT_COMMENTS_LOGGER = new_logger('log_format_comments.log')\n",
    "\n",
    "def unique_everseen(iterable, key=None):\n",
    "    \"\"\"List unique elements, preserving order. Remember all elements ever seen.\n",
    "    source: https://docs.python.org/3/library/itertools.html#itertools-recipes\"\"\"\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    if key is None:\n",
    "        for element in filterfalse(seen.__contains__, iterable):\n",
    "            seen_add(element)\n",
    "            yield element\n",
    "    else:\n",
    "        for element in iterable:\n",
    "            k = key(element)\n",
    "            if k not in seen:\n",
    "                seen_add(k)\n",
    "                yield element\n",
    "\n",
    "def dict_to_string(dictionary):\n",
    "    \"\"\"Flatten a dictionary into a single string\"\"\"\n",
    "    if isinstance(dictionary, dict):\n",
    "        return '\\n'.join([' says\\n'.join([key, value]) for key, value in dictionary.items()])\n",
    "    return None\n",
    "\n",
    "def parse_html_br_tag_content(break_tag):\n",
    "    \"\"\"Get content of the next and previous sibling of a <br/> tag\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    BeautifulSoup\n",
    "        BeautifulSoup object of <br/> tag\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "\n",
    "    Notes\n",
    "    ------\n",
    "    1. For each <br/> tag we run the .string method on its next and previous siblings\n",
    "    2. If a proper string is encountered, it is returned.\n",
    "    3. If <br/> is encountered, None is returned.\n",
    "    4. If any other string which result in an error is encountered, None is returned\n",
    "    \"\"\"\n",
    "    p_sibling = break_tag.previous_sibling\n",
    "    n_sibling = break_tag.next_sibling\n",
    "\n",
    "#     PARSE_BR_TAG_LOGGER.debug(\"previous sibling\\n{}\".format(p_sibling))\n",
    "#     PARSE_BR_TAG_LOGGER.debug(\"next sibling\\n{}\".format(n_sibling))\n",
    "\n",
    "    return_value = [None, None]\n",
    "    try:\n",
    "        return_value[0] = n_sibling.string.strip().strip(\"\\n:\")\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    try:\n",
    "        return_value[1] = p_sibling.string.strip().strip(\"\\n:\")\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    # return (\"*{}*, *{}*\".format(return_value[0], return_value[1]))\n",
    "    return tuple(return_value)\n",
    "\n",
    "def join_tuples(list_of_tuples):\n",
    "    \"\"\"Join a list of tuples into a list eliminating None and\n",
    "    duplicates.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    list\n",
    "        A list of tuples whose elements are strings.\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    list\n",
    "        A list of unique strings in the input list\n",
    "\n",
    "    Notes\n",
    "    ------\n",
    "    unique_everseen removes duplicates.\n",
    "    This is needed in cases where a next_sibling and a\n",
    "    previous_sibling point to the same string.\n",
    "    \"\"\"\n",
    "#     remove_nones = [[filter(lambda x: x is not None, each)] for each in list_of_tuples]\n",
    "    \n",
    "#     # a more explicit way\n",
    "#     remove_nones2 = []\n",
    "#     for each_tuple in list_of_tuples:\n",
    "#         l = []\n",
    "#         for each_string in each_tuple:\n",
    "#             if each_string is not None:\n",
    "#                 l.append(each_string)\n",
    "#         remove_nones2.append(l)\n",
    "#     print(remove_nones2)\n",
    "\n",
    "    # use list comprehension for speed\n",
    "    remove_nones = [\n",
    "        [each_string for each_string in each_tuple if each_string is not None] for each_tuple in list_of_tuples\n",
    "    ]\n",
    "    phrase_collection = [phrase.strip().strip(\"\\n:\") for each in remove_nones for phrase in each]\n",
    "    return \"\\n\".join(unique_everseen(phrase_collection))\n",
    "\n",
    "def format_comments(bs4_comment_block_object):\n",
    "    \"\"\"Format a comment block into proper paragraphs\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    BeautifulSoup\n",
    "        BeautifulSoup object of comment block\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    str\n",
    "        A properly paragraphed string\n",
    "    \"\"\"\n",
    "\n",
    "#     FORMAT_COMMENTS_LOGGER.debug(bs4_comment_block_object.prettify())\n",
    "\n",
    "    comment = []\n",
    "    break_tags = bs4_comment_block_object.find_all('br')\n",
    "\n",
    "    if break_tags == []:\n",
    "        return bs4_comment_block_object.text\n",
    "    for each in break_tags:\n",
    "        content = parse_html_br_tag_content(each)\n",
    "        comment.append(content)\n",
    "\n",
    "    return_string = join_tuples(comment)\n",
    "    return return_string\n",
    "\n",
    "def parse_comment_block(bs4_comment_block_object):\n",
    "    \"\"\"Return quoted string.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    BeautifulSoup\n",
    "        BeautifulSoup object of a quoted string block\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    quoted : OrderedDict()\n",
    "        quoted string content\n",
    "    bs4_comment_block_object : BeautifulSoup\n",
    "        Input comment block stripped of all <b> tags\n",
    "        \n",
    "    Notes\n",
    "    ------\n",
    "    Every comment block must be parsed with this function.\n",
    "    This function also has a side effect of producing a properly formatted html of all comments it encounters.\n",
    "    \"\"\"\n",
    "    \n",
    "#     PARSE_COMMENT_BLOCK_LOGGER.debug(bs4_comment_block_object.prettify())\n",
    "\n",
    "    save_dir = os.path.join(BASE_DIR, \"comment_block\")\n",
    "    if os.path.exists(save_dir) is False:\n",
    "        os.mkdir(save_dir)\n",
    "\n",
    "    # Side effect\n",
    "    save = os.path.join(save_dir, \"all_page_comments.html\")\n",
    "    with codecs.open(save, 'a+', encoding='utf-8') as f:\n",
    "        f.write(bs4_comment_block_object.prettify())\n",
    "        f.write(\"<div class='dropdown-divider'></div>\")\n",
    "\n",
    "    collected_quotes = OrderedDict()\n",
    "    return_val = namedtuple('ParsedComment', ['focus_user_comment', 'quotes_ordered_dict'])\n",
    "    blockquotes = bs4_comment_block_object.find_all('blockquote')\n",
    "\n",
    "    if blockquotes == []:\n",
    "        pass\n",
    "    else:\n",
    "        for blockquote in blockquotes:\n",
    "            try:\n",
    "                commenter = blockquote.find('b').text\n",
    "            except AttributeError:\n",
    "                commenter = 'Anonymous'\n",
    "\n",
    "            collected_quotes[commenter] = format_comments(blockquote).strip().strip(\"\\n:\")\n",
    "            blockquote.decompose() # remove the block from the tree\n",
    "\n",
    "    return_val.focus_user_comment = format_comments(bs4_comment_block_object).strip().strip(\"\\n:\")\n",
    "    return_val.quotes_ordered_dict = collected_quotes\n",
    "    return return_val\n",
    "\n",
    "def sort_dictionary_by_value(dictionary_to_sort):\n",
    "    \"\"\"\n",
    "    Return list of dictionary keys where the items are sorted on the values in descending order.\n",
    "    \n",
    "    e.g sort_dictionary_by_value({5:'goat', 10:'cat', 1:'dog'}) returns [5, 1, 10] since the values\n",
    "    sort to ['goat', 'dog', 'cat']\n",
    "    \"\"\"\n",
    "    if dictionary_to_sort is None:\n",
    "        return\n",
    "    ordered_dictionary = sorted(dictionary_to_sort.items(), key=itemgetter(1), reverse=True)\n",
    "    sorted_dictionary_list = [i[0] for i in ordered_dictionary]\n",
    "    return sorted_dictionary_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostCollector:\n",
    "    \"\"\"\n",
    "    Scrap a nairaland post\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    str\n",
    "        Post url\n",
    "    \"\"\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"PostCollector: {}\".format(self.base_url)\n",
    "\n",
    "    def __init__(self, base_url, refresh=True):\n",
    "        self.save_path = os.path.join(BASE_DIR, 'page_rips_post')\n",
    "        self.base_url = base_url # Page (0) of the post\n",
    "        self.refresh = refresh\n",
    "        self.title = self.base_url.split('/')[-1]\n",
    "        if not os.path.exists(self.save_path):\n",
    "            os.mkdir(self.save_path)\n",
    "\n",
    "    def max_page(self):\n",
    "        \"\"\"Returns the maximum number of pages of comments, starting from a zero index.\"\"\"\n",
    "        stop = 0\n",
    "        while True:\n",
    "            print(\"Whiling in a loop\")\n",
    "            if self._check_if_url_exists_and_is_valid(\"{}/{}\".format(self.base_url, stop)): # check if next url exists\n",
    "                stop += 1\n",
    "            else:\n",
    "                break\n",
    "        return stop\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_if_url_exists_and_is_valid(url): # ConnectionError happens here\n",
    "        r = requests.head(url)\n",
    "        return r.status_code == 200\n",
    "\n",
    "    def _scrap_comment_for_single_page(self, page_url):\n",
    "        \"\"\"Return comments and commenters on a single post page\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        OrderedDict\n",
    "            Dictionary of {commenter : comments}\n",
    "        \"\"\"\n",
    "\n",
    "        # User posts are contained in a table with summary='posts' attribute.\n",
    "        # Each commenter name is contained inside a <tr>\n",
    "        # Each comment is contained in <tr> just below the name of the commenter\n",
    "        soup = Ripper(page_url, parser='html5lib', save_path=self.save_path, refresh=self.refresh).soup\n",
    "\n",
    "        # Handle supposed anomaly by decomposing all such occurrences from the tree\n",
    "        for each in soup.find_all('td', class_=\"l pu pd\"):\n",
    "            each.parent.decompose()\n",
    "        rows = soup.find('table', summary='posts').find_all('tr')\n",
    "\n",
    "        return_val = OrderedDict()\n",
    "        for i in range(0, len(rows), 2):\n",
    "\n",
    "            topic_classes = ['bold l pu', 'bold l pu nocopy'] # topic div should be either of these classes\n",
    "            for class_ in topic_classes:\n",
    "                try:\n",
    "                    moniker = rows[i].find('td', class_=class_).find('a', href=True, class_=True).text.strip()\n",
    "                    break\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "                moniker = \"Nobody\" # set to nobody after exhausting all options. We cannot use finally in this case\n",
    "\n",
    "            comment_classes = ['l w pd', 'l w pd nocopy'] # comment div should be either of these classes            \n",
    "            for class_ in comment_classes:\n",
    "                try:\n",
    "                    comment_block = rows[i+1].find('td', id=True, class_=class_).find('div', class_='narrow')\n",
    "                    break\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "            parsed_block = parse_comment_block(comment_block)\n",
    "\n",
    "            # If a moniker already exists (i.e. a user has already commented), append an integer to the\n",
    "            # present one to differentiate both.\n",
    "            if moniker not in return_val:\n",
    "                return_val[moniker] = parsed_block\n",
    "            else:\n",
    "                moniker = \"{}**{}\".format(moniker, i)\n",
    "                return_val[moniker] = parsed_block\n",
    "        return return_val\n",
    "\n",
    "    def scrap_comments_for_range_of_pages(self, start=0, stop=1, __all=False):\n",
    "        \"\"\"Get contents for a range of pages from start to stop\"\"\"\n",
    "        if __all == True: # since we're starting from a zero index, we have to subtract 1 from self.max_page()\n",
    "            stop = self.max_page() - 1\n",
    "        while start <= stop:\n",
    "            next_url = \"{}/{}\".format(self.base_url, start)\n",
    "            next_page = self._scrap_comment_for_single_page(next_url)\n",
    "            yield next_page\n",
    "            start += 1\n",
    "\n",
    "    def all_commenters(self):\n",
    "        \"\"\"Return list of all commenters on a post\"\"\"\n",
    "        # Remember we user ** to separate a moniker and the number of times it is appearing on a post\n",
    "        return sorted([key.split(\"**\")[0] for each in list(self.scrap_comments_for_range_of_pages(stop=self.max_page())) for key, value in each.items()])\n",
    "\n",
    "    def unique_commenters(self):\n",
    "        \"\"\"Return list of unique commenters on a post\"\"\"\n",
    "        return sorted(set(self.all_commenters()))\n",
    "\n",
    "    def commenters_activity_summary(self):\n",
    "        \"\"\"Return count of number of times a user commented on a post\"\"\"\n",
    "        x = Counter(self.all_commenters())\n",
    "        print(x)\n",
    "        print(\"Sorted dict\")\n",
    "        print(sort_dictionary_by_value(x))\n",
    "        return \n",
    "\n",
    "class UserCommentHistory:\n",
    "    \"\"\"\n",
    "    Grab a user's comment history\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    user_name : str\n",
    "        User's name to crawl. Default is 'seun'\n",
    "    \"\"\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"UserCommentHistory: {}\".format(self.user_post_page)\n",
    "\n",
    "    def __init__(self, nairaland_moniker, refresh=True):\n",
    "        self.refresh = refresh\n",
    "        BASE_URL = 'https://www.nairaland.com'\n",
    "        self.save_path = os.path.join(BASE_DIR, 'page_rips_user')\n",
    "        if not os.path.exists(self.save_path):\n",
    "            os.mkdir(self.save_path)\n",
    "            \n",
    "        p = '{}/{}'.format(BASE_URL, nairaland_moniker.lower())\n",
    "        if self._check_if_url_exists_and_is_valid(p):\n",
    "            self.user_profile_page = p\n",
    "            self.user_post_page = '{}/{}/posts'.format(BASE_URL, nairaland_moniker.lower())\n",
    "        else:\n",
    "            raise NonExistentNairalandUser(\"This user does not exist on nairaland.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_if_url_exists_and_is_valid(url):\n",
    "        r = requests.head(url)\n",
    "        return r.status_code == 200\n",
    "    \n",
    "    def user_profile(self):\n",
    "        \"\"\"Returns a dictionary of the user's profile\"\"\"\n",
    "        pass\n",
    "\n",
    "    def max_pages(self):\n",
    "        \"\"\"Return number of pages of comment for user\"\"\"\n",
    "        soup = Ripper(self.user_post_page, save_path=self.save_path, refresh=True).soup\n",
    "        pattern = r\"\\<b\\>\\s*(\\d+)\\s*\\<\\/b\\>\" # pattern to search for number of pages of comments\n",
    "        try:\n",
    "            return int(re.search(pattern, str(soup)).group(1))\n",
    "        except AttributeError:\n",
    "            print(\"Could not find max page\")\n",
    "            pass\n",
    "\n",
    "    def _scrap_comment_for_single_page(self, page_url):\n",
    "        \"\"\"Return comments and commenters on a single post page\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        OrderedDict\n",
    "            Dictionary of {section : namedtuple}\n",
    "        \"\"\"\n",
    "\n",
    "        # User comments are contained in a table with neither summary nor id attribute.\n",
    "        # Then follows the rows containing the section, topic, and username and the comment itself just below it\n",
    "        soup = Ripper(page_url, parser='html5lib', save_path=self.save_path, refresh=self.refresh).soup\n",
    "\n",
    "        for each in soup.find_all('td', class_=\"l pu pd\"):\n",
    "            each.parent.decompose() # remove these trees as they are unneeded\n",
    "        rows = soup.find('table', id=False, summary=False).find_all('tr')\n",
    "\n",
    "        return_val = OrderedDict()\n",
    "        for i in range(0, len(rows), 2): # go to every second row\n",
    "            \n",
    "            topic_classes = ['bold l pu', 'bold l pu nocopy']\n",
    "            for class_ in topic_classes:\n",
    "                try:\n",
    "                    section_topic = rows[i].find('td', class_=class_).find_all('a', href=True, class_=False)\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "\n",
    "            comment_classes = ['l w pd', 'l w pd nocopy']\n",
    "            for class_ in comment_classes:\n",
    "                try:\n",
    "                    comment_block = rows[i+1].find('td', class_=class_).find('div', class_='narrow')\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "\n",
    "            section = section_topic[0].text.strip()\n",
    "            topic = section_topic[1].text.lstrip(\"Re:\").strip()\n",
    "\n",
    "            parsed_block = parse_comment_block(comment_block)\n",
    "\n",
    "            Comm = namedtuple('Comment', ['topic', 'parsed_comment'])\n",
    "            Comm.topic = topic\n",
    "            Comm.parsed_comment = parsed_block\n",
    "\n",
    "            if section not in return_val:\n",
    "                return_val[section] = Comm\n",
    "            else:\n",
    "                section = \"{}**{}\".format(section, i)\n",
    "                return_val[section] = Comm\n",
    "        return return_val\n",
    "\n",
    "    def scrap_comments_for_range_of_pages(self, start=0, stop=0, _maximum_pages=False):\n",
    "        \"\"\"Get contents for a range of pages from start to stop \"\"\"\n",
    "        if _maximum_pages:\n",
    "            stop = self.max_pages() - 1\n",
    "        while start <= stop:\n",
    "            next_url = \"{}/{}\".format(self.user_post_page, start)\n",
    "            next_page = self._scrap_comment_for_single_page(next_url)\n",
    "            yield next_page\n",
    "            start += 1\n",
    "\n",
    "class TopicCollector:\n",
    "    \"\"\"\n",
    "    Collect topics from a section of nairaland\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    section : str\n",
    "        Default section is politics\n",
    "\n",
    "    Notes\n",
    "    ------\n",
    "    The methods in this class are only applicable to section urls\n",
    "    \"\"\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"TopicCollector: {}\".format(self.base_url)\n",
    "\n",
    "    def __init__(self, section='politics'):\n",
    "        self.save_path = os.path.join(BASE_DIR, 'page_rips_section')\n",
    "        self.section = section\n",
    "        self.base_url = 'http://www.nairaland.com/{}'.format(self.section)\n",
    "        if os.path.exists(self.save_path) is False:\n",
    "            os.mkdir(self.save_path)\n",
    "\n",
    "    def max_pages(self):\n",
    "        \"\"\"Return number of pages in this section\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        int\n",
    "            The number of pages in this section\n",
    "        \"\"\"\n",
    "        soup = Ripper(self.base_url, parser='html5lib', save_path=self.save_path, refresh=True).soup\n",
    "        number = re.search(r\"\\(of\\s*(\\d+)\\s*pages\\)\", soup.text).group(1)\n",
    "        return int(number)\n",
    "\n",
    "    def _scrap_topics_for_a_single_page(self, page_url, refresh=True):\n",
    "        \"\"\"\n",
    "        Yield all topics on a page\n",
    "\n",
    "        Yields\n",
    "        -------\n",
    "        namedtuple\n",
    "            collection of 'poster', 'title', 'url', 'number of comments'\n",
    "        \"\"\"\n",
    "        soup = Ripper(page_url, parser='html5lib', save_path=self.save_path, refresh=True).soup\n",
    "        post_table = soup.find('table', id=False, summary=False)\n",
    "\n",
    "        for td in post_table.find_all('td', id=True):\n",
    "            Post = namedtuple('Post', ['poster', 'title', 'url', 'comments'])\n",
    "\n",
    "            title_component = td.find('b').find('a', href=True)\n",
    "            Post.title = title_component.text.strip()\n",
    "            Post.url = 'http://www.nairaland.com' + title_component.get('href').strip()\n",
    "\n",
    "            # there is a maximum of 7 <b> tags\n",
    "            meta_component = td.find('span', class_='s').find_all('b')\n",
    "\n",
    "            Post.poster = meta_component[0].text.strip()\n",
    "            Post.comments = int(meta_component[1].text) # count includes the post itself\n",
    "            yield Post\n",
    "\n",
    "    def scrap_topics_for_range_of_pages(self, start=0, end=0, _maximum_pages=False):\n",
    "        \"\"\"Yield all topics between 'start' and 'end' for a section\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        int\n",
    "            Start and end values of section\n",
    "\n",
    "        Yields\n",
    "        -------\n",
    "        tuple\n",
    "            same yields as for titles()\n",
    "        \"\"\"\n",
    "        if _maximum_pages:\n",
    "            end = self.max_pages() - 1\n",
    "        while start <= end:\n",
    "            next_url = '{}/{}'.format(self.base_url, start)\n",
    "            yield self._scrap_topics_for_a_single_page(next_url)\n",
    "            start += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_user_comments_to_html(username=None, max_page=5):\n",
    "    \"\"\"Export all of a user's comments data to a html file\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    str\n",
    "        Username\n",
    "    int\n",
    "        Maximum page count for user's comments (Default is 5 pages of comments)\n",
    "        loop breaks if we exceed actual count\n",
    "    \"\"\"\n",
    "    \n",
    "    if not username:\n",
    "        print(\"No username provided. Ending\")\n",
    "        return\n",
    "    else:\n",
    "        print(\"Now hacking nairaland. Please wait a few minutes.\")\n",
    "        \n",
    "    destination_file = os.path.join(BASE_DIR, \"comments_{}_{}_pages.html\".format(username.lower(), max_page))\n",
    "    if os.path.exists(destination_file):\n",
    "        os.remove(destination_file)\n",
    "    with open(destination_file, 'a+', encoding='utf-8') as f:\n",
    "        \n",
    "        # html scaffold\n",
    "        f.write(\"<html xmlns='http://www.w3.org/1999/xhtml'>\\n\")\n",
    "        f.write(\"<head>\\n\")\n",
    "        f.write(\"<link rel='stylesheet' href='https://stackpath.bootstrapcdn.com/bootswatch/4.1.3/superhero/bootstrap.min.css'>\\n\")\n",
    "        f.write(\"<meta name='viewport' content='width=device-width, initial-scale=1, shrink-to-fit=no'>\\n\")\n",
    "        f.write(\"<script src='https://stackpath.bootstrapcdn.com/bootstrap/4.1.1/js/bootstrap.min.js' integrity='sha384-smHYKdLADwkXOn1EmN1qk/HfnUcbVRZyYmZ4qpPea6sjB/pTJ0euyQp0Mk8ck+5T' crossorigin='anonymous' async></script>\\n\")\n",
    "        f.write(\"<title>Hack Nairaland - comment history for {}</title>\\n\".format(username.lower()))\n",
    "        f.write(\"</head>\\n\")\n",
    "        f.write(\"<body style='padding-top:5.5rem;'>\\n\")\n",
    "        # navbar\n",
    "        f.write(\"<nav class='navbar navbar-expand-lg navbar-dark bg-primary fixed-top'>\\n\")\n",
    "        f.write(\"<a class='navbar-brand'>Hack Nairaland</a>\\n\")\n",
    "        f.write(\"<button type='button' class='navbar-toggler my-toggler' data-toggle='collapse' data-target='.navcontent'>\\n\")\n",
    "        f.write(\"<span class='sr-only'>Toggle navigation</span>\\n\")\n",
    "        f.write(\"<span class='navbar-toggler-icon'></span>\\n\")\n",
    "        f.write(\"</button>\\n\")\n",
    "        f.write(\"<div class='collapse navbar-collapse navcontent'>\\n\")\n",
    "        f.write(\"<ul class='nav navbar-nav lefthand-navigation'>\\n\")\n",
    "        f.write(\"<li class='nav-item'><a class='nav-link' href='#' title='Home'>Home</a></li>\\n\")\n",
    "        f.write(\"</ul>\\n\")\n",
    "        f.write(\"</div>\\n\")\n",
    "        f.write(\"</nav>\\n\")\n",
    "        # end navbar\n",
    "        f.write(\"<div class='container'>\\n\")\n",
    "        f.write(\"<h1>Nairaland comment history for <a href='https://nairaland.com/{0}/posts' target='_blank'>{0}</a></h1>\\n\".format(username))\n",
    "        # breadcrumb\n",
    "        f.write(\"<nav aria-label='breadcrumb'>\\n\")\n",
    "        f.write(\"<ol class='breadcrumb'>\\n\")\n",
    "        f.write(\"<li class='breadcrumb-item'><a href='#'>Home</a></li>\\n\")\n",
    "        f.write(\"<li class='breadcrumb-item'>The first {} pages</li>\\n\".format(max_page))\n",
    "        f.write(\"</ol>\\n\")\n",
    "        f.write(\"</nav>\\n\")\n",
    "        # end breadcrumb\n",
    "\n",
    "        user = UserCommentHistory(username)\n",
    "        for page in list(user.scrap_comments_for_range_of_pages(stop=max_page)):\n",
    "            for section, topic_plus_comment in page.items():\n",
    "                f.write(\"<h3>{}</h3>\\n\".format(section.split('**')[0])) # remove the ** separating section and index\n",
    "                parsed_comment = topic_plus_comment.parsed_comment\n",
    "                f.write(\"<p class='text-success'>{}</p>\\n\".format(parsed_comment.focus_user_comment))\n",
    "                quotes = parsed_comment.quotes_ordered_dict\n",
    "                \n",
    "                for username, comment in quotes.items():\n",
    "                    f.write(\"<h4>{}</h4>\\n\".format(username))\n",
    "                    f.write(\"<p class='text-primary'><em>{}</em></p>\\n\".format(comment))\n",
    "                f.write(\"<div class='dropdown-divider'></div>\\n\")\n",
    "            f.write(\"<hr>\\n\")\n",
    "        f.write(\"</div>\\n\")\n",
    "        f.write(\"</body>\")    \n",
    "    print(\"Done hacking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do\n",
    "def export_user_comments_to_excel(username=None, max_page=5):\n",
    "    \"\"\"Export all of a user's comments data to excel\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    str\n",
    "        Username\n",
    "    int\n",
    "        Maximum page count for user's comments (Default is ..)\n",
    "        loop breaks if we exceed actual count\n",
    "    \"\"\"\n",
    "    if not username:\n",
    "        print(\"No username provided. Ending\")\n",
    "        return\n",
    "    else:\n",
    "        print(\"Now hacking nairaland. Please wait a few minutes.\")\n",
    "        \n",
    "    user_data = UserCommentHistory(username).scrap_comments_for_range_of_pages(stop=max_page)\n",
    "    user_comments = list(user_data)\n",
    "    work_book = OP.Workbook()\n",
    "    active_sheet = work_book.active\n",
    "    active_sheet.title = username\n",
    "    active_sheet['A1'] = \"SECTION\"\n",
    "    active_sheet['B1'] = \"TOPIC\"\n",
    "    active_sheet['C1'] = \"QUOTED\"\n",
    "    active_sheet['D1'] = \"USER COMMENT\"\n",
    "\n",
    "    user = UserCommentHistory(username)\n",
    "        for page in list(user.scrap_comments_for_range_of_pages(stop=max_page)):\n",
    "            for section, topic_plus_comment in page.items():\n",
    "                f.write(\"<h3>{}</h3>\\n\".format(section.split('**')[0])) # remove the ** separating section and index\n",
    "                parsed_comment = topic_plus_comment.parsed_comment\n",
    "                f.write(\"<p class='text-success'>{}</p>\\n\".format(parsed_comment.focus_user_comment))\n",
    "                quotes = parsed_comment.quotes_ordered_dict\n",
    "                \n",
    "                for username, comment in quotes.items():\n",
    "                    f.write(\"<h4>{}</h4>\\n\".format(username))\n",
    "                    f.write(\"<p class='text-primary'><em>{}</em></p>\\n\".format(comment))\n",
    "                f.write(\"<div class='dropdown-divider'></div>\\n\")\n",
    "            f.write(\"<hr>\\n\")\n",
    "        f.write(\"</div>\\n\")\n",
    "        f.write(\"</body>\")    \n",
    "    print(\"Done hacking\")\n",
    "\n",
    "    row_number = 2\n",
    "    for each_comment in user_comments:\n",
    "\n",
    "        poster = each_comment[0]\n",
    "        section = each_comment[1]\n",
    "        topic = each_comment[2]\n",
    "        quoted = single_string_from_dictionary(each_comment[3])\n",
    "        comment = each_comment[4]\n",
    "\n",
    "        active_sheet.cell(row=row_number, column=1, value=poster)\n",
    "        active_sheet.cell(row=row_number, column=2, value=section)\n",
    "        active_sheet.cell(row=row_number, column=3, value=topic)\n",
    "        active_sheet.cell(row=row_number, column=4, value=quoted)\n",
    "        active_sheet.cell(row=row_number, column=5, value=comment)\n",
    "\n",
    "        row_number += 1\n",
    "    work_book.save(os.path.join(BASE_DIR, \"comments_{}_{}_pages.xlsx\".format(username.lower(), max_page)))\n",
    "\n",
    "def title_word_count(section='romance', max_page=10):\n",
    "    \"\"\"Group words that occur in titles of a section\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    str\n",
    "        Section\n",
    "    int\n",
    "        Maximum number of section pages to scrap\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    word_count : list\n",
    "        Dictionary of each word and its count\n",
    "    list\n",
    "        A list of all words sorted in descending order of frequency\n",
    "    \"\"\"\n",
    "    word_list = []\n",
    "    section_object = TopicCollector(section=section).titles_links_metadata(end=max_page)\n",
    "    section_data = list(section_object)\n",
    "    split_string = r\"[\\; \\, \\n \\.+\\- \\( \\) - \\/ : \\? \\[ \\] \\ — –]\"\n",
    "\n",
    "    exclude = ['to', 'a', 'as', 'the', 'you', 'in', 'is', 'i', 'with', 'of', 'an', 'and', 'my',\n",
    "               'your', 'for', 'on', 'what', 'her', 'this', 'that', 'these', 'those', 'me']\n",
    "\n",
    "    for each in section_data:\n",
    "        title = each[3]\n",
    "        words_in_title = re.split(split_string, title)\n",
    "        word_list.extend(words_in_title)\n",
    "\n",
    "    word_list = [x.lower() for x in word_list]\n",
    "    word_list = filter(lambda x: x not in exclude, word_list)\n",
    "    word_count = Counter(list(word_list))\n",
    "    return word_count, sort_dictionary_by_value(word_count)\n",
    "\n",
    "def export_topics_to_excel(section='romance', start_page=0, end_page=3):\n",
    "    \"\"\"Writes all topics between start and end of a section to excel.\n",
    "    Same output as titles_links_metadata() but written to a excel file\n",
    "    \"\"\"\n",
    "    work_book = OP.Workbook()\n",
    "    active_sheet = work_book.active\n",
    "    active_sheet.title = section\n",
    "    active_sheet['A1'] = \"POSTER\"\n",
    "    active_sheet['B1'] = \"MONTH\"\n",
    "    active_sheet['C1'] = \"YEAR\"\n",
    "    active_sheet['D1'] = \"TITLE\"\n",
    "    active_sheet['E1'] = \"WEB URL\"\n",
    "\n",
    "    section_object = TopicCollector(section=section)\n",
    "    section_titles = section_object.titles_links_metadata(start=start_page, end=end_page)\n",
    "    section_data = list(section_titles)\n",
    "\n",
    "    row_number = 2\n",
    "\n",
    "    for each in section_data:\n",
    "        title = each[0]\n",
    "        link = each[1]\n",
    "        month = each[2]\n",
    "        year = each[3]\n",
    "        poster = each[4]\n",
    "\n",
    "        active_sheet.cell(row=row_number, column=1, value=title)\n",
    "        active_sheet.cell(row=row_number, column=2, value=link)\n",
    "        active_sheet.cell(row=row_number, column=3, value=month)\n",
    "        active_sheet.cell(row=row_number, column=4, value=year)\n",
    "        active_sheet.cell(row=row_number, column=5, value=poster)\n",
    "        row_number += 1\n",
    "\n",
    "    fname = \"{}_topics_{}_to_{}.xlsx\".format(section, start_page, end_page)\n",
    "    work_book.save(os.path.join(BASE_DIR, fname))\n",
    "\n",
    "def export_post_to_docx_format(post_url):\n",
    "    \"\"\"Export post to word\"\"\"\n",
    "\n",
    "    post = PostCollector(post_url)\n",
    "    comments_and_commenters = post.view_comments_commenters()\n",
    "\n",
    "    document = docx.Document()\n",
    "    document.add_paragraph(post_url)\n",
    "\n",
    "    for each in comments_and_commenters:\n",
    "        commenter = each[0]\n",
    "        comment_block = each[1]\n",
    "\n",
    "        quotes = single_string_from_dictionary(comment_block[0])\n",
    "        user_comments = comment_block[1]\n",
    "\n",
    "        document.add_paragraph().add_run(commenter).bold = True\n",
    "        document.add_paragraph().add_run('[{}]'.format(quotes)).italic = True\n",
    "        document.add_paragraph(user_comments)\n",
    "        document.add_paragraph('*'*50)\n",
    "\n",
    "    fname = \"{}.docx\".format(post.post_title)\n",
    "    document.save(os.path.join(BASE_DIR, fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "export_user_comments_to_html(username=\"seun\", max_page=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change the cell type to code to run this cell and view the output\n",
    "\n",
    "import textwrap\n",
    "\n",
    "user_comments = UserCommentHistory(\"preccy69\")\n",
    "for page in list(user_comments.scrap_comments_for_range_of_pages(start=0, stop=1)):\n",
    "    for section, topic_plus_comment in page.items():\n",
    "        print(\"\\n\\n\", \"*\"*40, section, \"*\"*40)\n",
    "        print(topic_plus_comment.topic.upper()) # for differentiation only\n",
    "\n",
    "        parsed_comment = topic_plus_comment.parsed_comment # a namedtuple instance\n",
    "        print(parsed_comment.focus_user_comment)\n",
    "\n",
    "        quotes = parsed_comment.quotes_ordered_dict\n",
    "        for username, comment in quotes.items():\n",
    "            print(\" \"*8)\n",
    "            print(textwrap.indent(username, \"    \"))\n",
    "            print(textwrap.indent(comment, \"    \"))\n",
    "            \n",
    "        print(\"_\"*100)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change the cell type to code to run this cell and view the output\n",
    "\n",
    "import textwrap\n",
    "p = TopicCollector(section='politics')\n",
    "for page in p.scrap_topics_for_range_of_pages(end=1):\n",
    "    for topic in list(page):\n",
    "        print(topic.poster)\n",
    "        print(textwrap.indent(topic.title, \"    \"))\n",
    "        print(textwrap.indent(topic.url, \"    \"))\n",
    "        print(textwrap.indent(str(topic.comments), \"    \"), \" comments\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hack nairaland",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
